# Deep Learning from Scratch2
**『밑바닥부터 시작하는 딥러닝 ❷』(한빛미디어, 2019)**

> [저자 Github](https://github.com/oreilly-japan/deep-learning-from-scratch-20)   
> [번역본 Github](https://github.com/WegraLee/deep-learning-from-scratch-2)

> 스터디 시작일자 : 2019.12.15

> 스터디 목적 : 자연어 처리에 관해 심화 학습하면서 밑바닥부터 구현해보기

<br/>
<p align="left">
<img src ="http://www.hanbit.co.kr/data/books/B8950212853_l.jpg" height="250px"/>
<!-- #</p> -->
<br/>

## 목차

|   | Chapter                   | 개인 진도   | 발제 |
|---|---------------------------|----------|-------|
| 1 | 신경망 복습                  | 19.12.18 |       |
| 2 | 자연어와 단어의 분산 표현       | 19.12.27 |  백승주  |
| 3 | word2vec                  | 20.1.7   | 김우정  |
| 4 | word2vec 속도 개선          | 20.2.22  |  조원익  |
| 5 | 순환 신경망 (RNN)            |          |  운봉영  |
| 6 | 게이트가 추가된 RNN           |          | 김희아   |
| 7 | RNN을 사용한 문장 생성        |          |  백승주  |
| 8 | 어텐션                     |          |  김우정  |

## 참여자
슬기, 봉영, 원익, 승주, 우정, 희아

## 스터디 일지
**Week 1.** 스터디 시작 
- 각 챕터의 발제자를 정해서 오프라인 모임 때 발표 
- Ch 1. 신경망 복습 - 각자 공부 
   
**Week 2.** Chapter 2 - Chapter 3
- Ch 2. 자연어와 단어의 분산 표현
    - SVD의 개념 심화 논의 
        - 2-1. eigenvalue 의미?
        - 2-2. singular vector -> 중요도 큰 값이 왜 중요한 것인가??
- Ch 3. word2vec

**Week 3.** Chapter 4 - Chapter 5 
- Ch 4. word2vec 속도 개선
    - analogy test의 원리에 관해 심화 공부 진행
    - Ethayarajh et al., Towards Understanding Linear Word Analogies, ACL 209
- Ch 5. RNN


