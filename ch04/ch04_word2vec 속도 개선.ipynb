{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**3ì¥ì˜ simple word2vecì— ë‘ ê°€ì§€ ê°œì„  ë°©ì•ˆ ì¶”ê°€**\n",
    "\n",
    "1. Embeddingì´ë¼ëŠ” ìƒˆë¡œìš´ ê³„ì¸µ ë„ì…\n",
    "\n",
    "2. ë„¤ê±°í‹°ë¸Œ ìƒ˜í”Œë§ì´ë¼ëŠ” ìƒˆë¡œìš´ ì†ì‹¤ í•¨ìˆ˜ ë„ì…"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "toc-hr-collapsed": false
   },
   "source": [
    "# word2vec ê°œì„  (1)\n",
    "\n",
    "3ì¥ì—ì„œì˜ CBOW ëª¨ë¸ì€ ë‹¤ë£¨ëŠ” ì–´íœ˜ì˜ ìˆ˜ê°€ ì´ 7ê°œì´ê¸°ì— ë¬¸ì œì—†ì´ ì‘ë™í•œë‹¤. í•˜ì§€ë§Œ ê±°ëŒ€í•œ ë§ë­‰ì¹˜ë¥¼ ë‹¤ë£¨ê²Œ ë˜ë©´ ë¬¸ì œê°€ ë°œìƒí•œë‹¤.  \n",
    "\n",
    "ì–´íœ˜ê°€ 100ë§Œ ê°œ, ì€ë‹‰ì¸µì˜ ë‰´ëŸ°ì´ 100ê°œì¸ CBOWë¥¼ ìƒê°í•´ë³´ì. ì…ë ¥ì¸µê³¼ ì¶œë ¥ì¸µì—ëŠ” ê° 100ë§Œ ê°œì˜ ë‰´ëŸ°ì´ ì¡´ì¬í•œë‹¤.\n",
    "\n",
    "#### ì´ ë•Œ ë‹¤ìŒì˜ ë‘ ê°€ì§€ ë³‘ëª©í˜„ìƒì´ ë°œìƒí•œë‹¤.\n",
    "\n",
    "\n",
    "- **ì…ë ¥ì¸µì˜ ì›í•« í‘œí˜„ê³¼ ê°€ì¤‘ì¹˜ í–‰ë ¬ $W_{in}$ ì˜ ê³± ê³„ì‚°**\n",
    "  - ì›í•« í‘œí˜„ê³¼ ê´€ë ¨ëœ ë¬¸ì œ\n",
    "  - `Embedding`ì´ë¼ëŠ” ìƒˆë¡œìš´ ê³„ì¸µ ë„ì…\n",
    "  \n",
    "        \n",
    "- **ì€ë‹‰ì¸µê³¼ ê°€ì¤‘ì¹˜ í–‰ë ¬ $W_{out}$ì˜ ê³± ë° Softmax ê³„ì¸µì˜ ê³„ì‚°**\n",
    "  - ì€ë‹‰ì¸µ ì´í›„ì˜ ê³„ì‚° ë¬¸ì œ\n",
    "  - ì€ë‹‰ì¸µê³¼ ê°€ì¤‘ì¹˜ í–‰ë ¬ $W_{out}$ì˜ ê³±ì˜ ê³„ì‚°ëŸ‰ì´ ìƒë‹¹í•¨\n",
    "  - `negative sampling` ì´ë¼ëŠ” ìƒˆë¡œìš´ ì†ì‹¤ í•¨ìˆ˜ ë„ì…\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Embedding ê³„ì¸µ\n",
    "\n",
    "ì–´íœ˜ì˜ ìˆ˜ê°€ 100ë§Œ ê°œ, ì€ë‹‰ì¸µì˜ ë‰´ëŸ°ì´ 100ê°œì¸ ê²½ìš°ë¥¼ ê°€ì •í•´ë³´ì.\n",
    "\n",
    "$c * W_{in} = h$ ì¼ ë•Œ, í˜•ìƒì€ ë‹¤ìŒê³¼ ê°™ë‹¤.\n",
    "\n",
    "- c.shape = (1, 1000000)\n",
    "- W.shape = (1000000, 100)\n",
    "- h.shape = (1,100)\n",
    "\n",
    "**> ì´ëŠ” ê²°ê³¼ì ìœ¼ë¡œ $W_{in}$ í–‰ë ¬ì—ì„œ íŠ¹ì • í–‰ì„ ì¶”ì¶œí•˜ëŠ” ê²ƒ ë¿ì´ë‹¤.**   \n",
    "ì—¬ê¸°ì„œ ë§í•˜ëŠ” \"íŠ¹ì • í–‰\"ì´ë¼ í•¨ì€ $W_{in}$ í–‰ë ¬ì—ì„œ input c ë‹¨ì–´ì˜ idì— í•´ë‹¹í•˜ëŠ” ì¸ë±ìŠ¤ í–‰ì„ ë§í•¨\n",
    "\n",
    "### ì¦‰, ì›í•« í‘œí˜„ê³¼ MatMul ê³„ì¸µì˜ í–‰ë ¬ê³± ì—°ì‚°ì€ ì‚¬ì‹¤ ìƒ ë¶ˆí•„ìš”í•œ ì‘ì—…ì„!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "ê·¸ëŸ¬ë©´ ê°€ì¤‘ì¹˜ ë§¤ê°œë³€ìˆ˜ë¡œë¶€í„° `ë‹¨ì–´ IDì— í•´ë‹¹í•˜ëŠ” í–‰(ë²¡í„°)`ë¥¼ ì¶”ì¶œí•˜ëŠ” ê³„ì¸µì„ ë§Œë“¤ì–´ë³´ì!<br>\n",
    "<u>ì—¬ê¸°ì„œì˜ ê³„ì¸µì´ ë°”ë¡œ Embedding ê³„ì¸µ!</u>\n",
    "\n",
    "**ì°¸ê³ ) Embeddingì€ ë‹¨ì–´ ì„ë² ë”©ì´ë¼ëŠ” ìš©ì–´ì—ì„œ ìœ ë˜í–ˆë‹¤.\n",
    "\n",
    "ì¦‰, ìš°ë¦¬ê°€ í•  ì¼ì€ Embedding ê³„ì¸µì— ë‹¨ì–´ ì„ë² ë”© (ë¶„ì‚° í‘œí˜„)ì„ ì €ì¥í•˜ëŠ” ê²ƒì´ë‹¤.\n",
    "\n",
    "`Q?` ê¸°ì¡´ì˜ Wì— ì €ì¥ë  ë¶„ì‚° í‘œí˜„ì´ Embedding ê³„ì¸µìœ¼ë¡œ ì˜®ê²¨ê°„ ê²ƒ?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Embedding ê³„ì¸µ êµ¬í˜„"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 0,  1,  2],\n",
       "       [ 3,  4,  5],\n",
       "       [ 6,  7,  8],\n",
       "       [ 9, 10, 11],\n",
       "       [12, 13, 14],\n",
       "       [15, 16, 17],\n",
       "       [18, 19, 20]])"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "W = np.arange(21).reshape(7,3)\n",
    "W"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([6, 7, 8])"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "array([15, 16, 17])"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# ë‹¨ì¼ í–‰ ë½‘ê¸°\n",
    "# idx = 2\n",
    "display(W[2])\n",
    "# idx = 5\n",
    "display(W[5])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 3,  4,  5],\n",
       "       [ 0,  1,  2],\n",
       "       [ 9, 10, 11],\n",
       "       [ 0,  1,  2]])"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# idx ì—¬ëŸ¬ê°œ í•œë²ˆì— ì¶”ì¶œ\n",
    "idx = np.array([1,0,3,0])\n",
    "W[idx]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Embedding: \n",
    "    def __init__(self,W):\n",
    "        self.params = [W]\n",
    "        self.grads = [np.zeros_like(W)]\n",
    "        self.idx = None # ë‹¨ì–´ ID ì¸ë±ìŠ¤ë¥¼ ë°°ì—´ë¡œ ì €ì¥, ë¯¸ë‹ˆë°°ì¹˜ë¥¼ ê³ ë ¤í•  ë•Œ ì—¬ëŸ¬ê°œë„ ì¶”ì¶œë  ìˆ˜ ìˆë„ë¡ êµ¬í˜„\n",
    "        \n",
    "    # ê°€ì¤‘ì¹˜ Wì˜ íŠ¹ì • í–‰ì„ ì¶”ì¶œ\n",
    "    def forward(self, idx):\n",
    "        W, = self.params\n",
    "        self.idx = idx\n",
    "        out = W[idx] \n",
    "        return out\n",
    "    \n",
    "    # ë°˜ëŒ€ë¡œ ì—­ì „íŒŒì—ì„œëŠ” ë°˜ëŒ€ë¡œ ì¶œë ¥ì¸µì—ì„œì˜ ê¸°ìš¸ê¸°ë¥¼ Wì˜ idx í–‰ì— í• ë‹¹\n",
    "    def backward(self, dout):\n",
    "        dW, = self.grads # ê°€ì¤‘ì¹˜ ê¸°ìš¸ê¸° dWë¥¼ êº¼ë‚¸ë‹¤\n",
    "        dW[...] = 0 # dWì˜ í˜•ìƒì„ ìœ ì§€í•œ ì±„ ì›ì†Œë§Œ 0ìœ¼ë¡œ ë®ì–´ì“´ë‹¤\n",
    "        \n",
    "        # idx ì¤‘ë³µì„ ê³ ë ¤í•˜ì§€ ì•Šì€ ë°©ë²•\n",
    "        # dW[self.idx] = dout # ì• ì¸µì—ì„œ ì „í•´ì§„ ê¸°ìš¸ê¸° doutì„ idx ë²ˆì§¸ í–‰ì— í• ë‹¹í•œë‹¤. \n",
    "        \n",
    "        # idx ì¤‘ë³µ ê³ ë ¤ : í• ë‹¹ x ë”í•˜ê¸° !\n",
    "        # Method #1 ëŠë¦¬ë‹¹\n",
    "        #for i, word_id in enumerate(self.idx):\n",
    "        #    dW[word_id] += dout[i]\n",
    "        \n",
    "        # Method #2 íš¨ìœ¨ì . np.add.at(A,idx,B) = Bë¥¼ Aì˜ idxë²ˆì§¸ í–‰ì— ë”í•´ì¤€ë‹¤.\n",
    "        np.add.at(dW, self.idx, dout)\n",
    "        \n",
    "        return None"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**[ì£¼ì˜]** ì—­ì „íŒŒ ê³¼ì •ì—ì„œ idxì˜ ì¤‘ë³µì„ ê³ ë ¤í•˜ì—¬ ê°’ì„ 'í• ë‹¹'í•˜ëŠ” ê²ƒì´ ì•„ë‹ˆë¼ ê°’ì„ **ë”í•˜ê¸°** í•œë‹¤.\n",
    "\n",
    "### ë‹¨ì \n",
    " - ë‹¤ì˜ì–´ ë¬¸ì œ í•´ê²° ë¶ˆê°€\n",
    " - ë‹¨ì–´ì˜ ìœ„ì¹˜ ê³ ë ¤ê°€ ë˜ì§€ ì•ŠìŒ\n",
    " \n",
    "**-> ê¶ê·¹ì ìœ¼ë¡œ ë¬¸ì¥ ë‹¨ìœ„ë¡œ ê°€ì•¼ í•˜ëŠ” ì´ìœ  (Elmo, Transformer)**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Embedding ê³„ì¸µ êµ¬í˜„ìœ¼ë¡œ ì¸í•œ ì´ì **\n",
    "\n",
    ": ì…ë ¥ ì¸¡ MatMul ê³„ì¸µì„ Embedding ê³„ì¸µìœ¼ë¡œ ì „í™˜í•˜ì—¬ ë©”ëª¨ë¦¬ ì‚¬ìš©ëŸ‰ì„ ì¤„ì´ê³  ì“¸ë°ì—†ëŠ” ê³„ì‚°ëŸ‰ì„ ìƒëµ!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# word2vec ê°œì„  (2)\n",
    "\n",
    "ì€ë‹‰ì¸µ ì´í›„, <u>í–‰ë ¬ ê³±ê³¼ Softmax ê³„ì¸µì˜ ê³„ì‚°</u>ì—ì„œ ë°œìƒí•˜ëŠ” ë³‘ëª© í˜„ìƒì„ í•´ê²°í•´ë³´ì\n",
    "\n",
    "by **`ë„¤ê±°í‹°ë¸Œ ìƒ˜í”Œë§ ê¸°ë²•`**\n",
    "\n",
    "Softmax ëŒ€ì‹  ë„¤ê±°í‹°ë¸Œ ìƒ˜í”Œë§ì„ ì´ìš©í•˜ë©´ ì–´íœ˜ê°€ ì•„ë¬´ë¦¬ ë§ì•„ì ¸ë„ ê³„ì‚°ëŸ‰ì„ ì–µì œí•  ìˆ˜ ìˆë‹¤.\n",
    "\n",
    "ì–´ë µë‹¤! ë³µì¡í•˜ë‹¤!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ì€ë‹‰ì¸µ ì´í›„ ê³„ì‚°ì˜ ë¬¸ì œì \n",
    "\n",
    "ë§ˆì°¬ê°€ì§€ë¡œ ì–´íœ˜ê°€ 100ë§Œ ê°œ, ì€ë‹‰ì¸µ ë‰´ëŸ°ì˜ ìˆ˜ê°€ 100ê°œì¸ CBOW ëª¨ë¸ì„ ê°€ì •í•´ë³´ì\n",
    "\n",
    "<img src=\"../imgs/fig 4-2.png\" width=\"500\" align='left'>\n",
    "<br></br>\n",
    "\n",
    "**ë¬¸ì œì  1. ì€ë‹‰ì¸µì˜ ë‰´ëŸ° X $W_{out}$**\n",
    "\n",
    "    - ì€ë‹‰ì¸µ ë²¡í„°ì˜ í¬ê¸°ê°€ 100\n",
    "    - ê°€ì¤‘ì¹˜ í–‰ë ¬ì˜ í¬ê¸°ëŠ” 100 X 100ë§Œ\n",
    "<br>\n",
    "\n",
    "**ë¬¸ì œì  2. Softmax ê³„ì¸µì˜ ê³„ì‚°**\n",
    "    \n",
    "    - Softmaxì˜ ê³„ì‚°ì‹ì—ì„œ ë¶„ëª¨ì˜ ê°’ì„ ì–»ê¸° ìœ„í•´ Exponential ê³„ì‚°ì„ 100ë§Œ ë²ˆ ìˆ˜í–‰í•´ì•¼ í•œë‹¤...!\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ë‹¤ì¤‘ ë¶„ë¥˜ì—ì„œ ì´ì§„ ë¶„ë¥˜ë¡œ\n",
    "\n",
    "ë„¤ê±°í‹°ë¸Œ ìƒ˜í”Œë§ ê¸°ë²•ì˜ í•µì‹¬ ì•„ì´ë””ì–´ëŠ” `ì´ì§„ ë¶„ë¥˜ (binary classification)` ì— ìˆë‹¤.   \n",
    "\n",
    "ì¦‰, **<u>ë‹¤ì¤‘ ë¶„ë¥˜ë¥¼ ì´ì§„ ë¶„ë¥˜ë¡œ ê·¼ì‚¬í•˜ëŠ” ê²ƒ</u>**\n",
    "\n",
    "    simple_CBOW ì—ì„œì˜ ë°©ì‹ : 100ë§Œ ê°œì˜ ë‹¨ì–´ ì¤‘ ì˜³ì€ ë‹¨ì–´ í•˜ë‚˜ë¥¼ ì„ íƒí•˜ëŠ” ë¬¸ì œ   \n",
    "\n",
    "        you, goodbye ---> íƒ€ê¹ƒë‹¨ì–´ëŠ” ë¬´ì—‡ì¸ê°€? ---> say (1/100ë§Œ)\n",
    "\n",
    "    ë„¤ê±°í‹°ë¸Œ ìƒ˜í”Œë§ ë°©ì‹ : 100ë§Œ ê°œì˜ ë‹¨ì–´ ì¤‘ íŠ¹ì • ë‹¨ì–´ê°€ ì˜³ì€ ë‹¨ì–´ì¸ì§€ ì•„ë‹Œì§€ ì´ì§„ íŒë‹¨í•˜ëŠ” ë¬¸ì œ\n",
    "\n",
    "        you, goodbye ---> sayì¸ê°€? (Yes, No)\n",
    "\n",
    "ë³¸ ë°©ë²•ì„ êµ¬í˜„í•˜ê¸° ìœ„í•´ì„œëŠ” ?\n",
    "#### > ì¶œë ¥ì¸µì— ë‰´ëŸ°ì„ í•˜ë‚˜ë§Œ ì¤€ë¹„í•˜ë©´ ëœë‹¤. ì¶œë ¥ì¸µì˜ ë‰´ëŸ°ì´ `say`ì˜ ì ìˆ˜ë¥¼ ì¶œë ¥í•˜ëŠ” ê²ƒì´ë‹¤. \n",
    "\n",
    "`ì´ì „ì—ëŠ” ì¶œë ¥ì¸µì— softmaxë¡œ ëª¨ë“  ë‹¨ì–´ì— ëŒ€í•œ í™•ë¥ ì„ ì¶œë ¥í•œ ë’¤, ê°€ì¥ ë†’ì€ í™•ë¥ ì˜ ë‹¨ì–´ë¥¼ ì˜ˆì¸¡í•˜ëŠ” ì‹ìœ¼ë¡œ ê³„ì‚°ì„ ìˆ˜í–‰í–ˆë‹¤ë©´,    \n",
    "ì´ì œëŠ” ì¶œë ¥ì¸µì˜ sayì— í•´ë‹¹í•˜ëŠ” ë‹¨ì–´ì— sigmoid í•¨ìˆ˜ë¥¼ ì ìš©ì‹œì¼œ sayì— í•´ë‹¹í•˜ëŠ” í™•ë¥ ê°’ë§Œì„ ë°›ì•„ì˜¨ë‹¤.\n",
    "ëª¨ë“  ë‹¨ì–´ì— ëŒ€í•œ ê³„ì‚°ì„ ë‹¨ì¼ ë‹¨ì–´ì— ëŒ€í•œ ê³„ì‚°ìœ¼ë¡œ ë³€í™˜`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ì‹œê·¸ëª¨ì´ë“œ í•¨ìˆ˜ì™€ êµì°¨ ì—”íŠ¸ë¡œí”¼ ì˜¤ì°¨\n",
    "\n",
    "sigmoid í•¨ìˆ˜ : \n",
    "\n",
    "$ y = \\frac{1}{1 + exp(-x)} $\n",
    "\n",
    "êµì°¨ ì—”íŠ¸ë¡œí”¼ ì˜¤ì°¨ : \n",
    "\n",
    "$L = -(tlogy + (1-t)log(1-y))$\n",
    "\n",
    " - y : ì‹œê·¸ëª¨ì´ë“œ í•¨ìˆ˜ì˜ ì¶œë ¥\n",
    " - t : ì •ë‹µ ë ˆì´ë¸” (1 í˜¹ì€ 0)\n",
    " - t = 1  > ì •ë‹µ > -logy\n",
    " - t = 0 > ì •ë‹µ ì•„ë‹˜ > -log(1-y)\n",
    " \n",
    "**Sigmoid + Cross Entropy Errorë¥¼ ì¡°í•©í•˜ì—¬ ì—­ì „íŒŒì˜ ê°’ì´ y-t ë¼ëŠ” ê°’ìœ¼ë¡œ ê¹”ë”í•˜ê²Œ ë„ì¶œëœë‹¤!**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ë‹¤ì¤‘ ë¶„ë¥˜ì—ì„œ ì´ì§„ ë¶„ë¥˜ë¡œ (êµ¬í˜„)\n",
    "\n",
    "Embed -> hidden -> Embed dot -> Sigmoid with loss\n",
    "\n",
    "** Embedding Dot Layer : Embedding Layer + dot product"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "class EmbeddingDot:\n",
    "    def __init__(self, W):\n",
    "        # ì´ 4ê°œì˜ ì¸ìŠ¤í„´ìŠ¤ ë³€ìˆ˜\n",
    "        self.embed = Embedding(W) # Embedding ê³„ì¸µ\n",
    "        self.params = self.embed.params # ë§¤ê°œë³€ìˆ˜ ì €ì¥\n",
    "        self.grads = self.embed.grads # ê¸°ìš¸ê¸° ì €ì¥\n",
    "        self.cache = None # ìˆœì „íŒŒ ì‹œì˜ ê³„ì‚° ê²°ê³¼ë¥¼ ì ì‹œ ìœ ì§€í•˜ê¸° ìœ„í•´ ì‚¬ìš©ë˜ëŠ” ë³€ìˆ˜ \n",
    "        \n",
    "    # ìˆœì „íŒŒ ë©”ì„œë“œì—ì„œëŠ” ì€ë‹‰ì¸µ ë‰´ëŸ°ê³¼ ë‹¨ì–´ IDì˜ ë„˜íŒŒì´ ë°°ì—´(ë¯¸ë‹ˆë°°ì¹˜)ì„ ë°›ëŠ”ë‹¤.\n",
    "    def forward(self, h, idx):\n",
    "        target_W = self.embed.forward(idx) # embedding ê³„ì¸µì˜ forward(idx)ë¥¼ í˜¸ì¶œí•˜ì—¬ idxì— í•´ë‹¹í•˜ëŠ” í–‰ ì¶”ì¶œ\n",
    "        out = np.sum(target_W * h, axis = 1) # ë‚´ì  ê³„ì‚° ì´í›„ í–‰ë§ˆë‹¤ ë”í•˜ì—¬ ìµœì¢…ê²°ê³¼ out ë°˜í™˜\n",
    "        \n",
    "        self.cache = (h, target_W)\n",
    "        return out\n",
    "    \n",
    "    def backward(self, dout):\n",
    "        h, target_W = self.cache\n",
    "        dout = dout.reshape(dout.shape[0],1)\n",
    "        \n",
    "        dtarget_W = dout * h\n",
    "        self.embed.backward(dtarget_W)\n",
    "        dh = dout * target_W\n",
    "        return dh"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 0,  1,  2],\n",
       "       [ 3,  4,  5],\n",
       "       [ 6,  7,  8],\n",
       "       [ 9, 10, 11],\n",
       "       [12, 13, 14],\n",
       "       [15, 16, 17],\n",
       "       [18, 19, 20]])"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "W = np.arange(21).reshape(7,3)\n",
    "W"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 0,  1,  2],\n",
       "       [ 9, 10, 11],\n",
       "       [ 3,  4,  5]])"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "idx = [0,3,1]\n",
    "target_W = W[idx]\n",
    "target_W"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0, 1, 2],\n",
       "       [3, 4, 5],\n",
       "       [6, 7, 8]])"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "h = np.arange(9).reshape(3,3)\n",
    "h"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 0,  1,  4],\n",
       "       [27, 40, 55],\n",
       "       [18, 28, 40]])"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "target_W * h"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([  5, 122,  86])"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.sum(target_W * h, axis = 1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ë„¤ê±°í‹°ë¸Œ ìƒ˜í”Œë§\n",
    "\n",
    "í˜„ì¬ê¹Œì§€ëŠ” ì •ë‹µì— ëŒ€í•´ì„œë§Œ í•™ìŠµ.    \n",
    "ì˜ˆë¥¼ ë“¤ì–´, `you`ì™€ `goodbye` ê°€ ë§¥ë½ ì…ë ¥ìœ¼ë¡œ ë“¤ì–´ê°”ì„ ë•Œ ì •ë‹µ ë ˆì´ë¸”ì´ `say`ì¸ ê²½ìš°, ì¢‹ì€ ê°€ì¤‘ì¹˜ê°€ ì¤€ë¹„ë˜ì–´ ìˆë‹¤ë©´ Sigmoid ê³„ì¸µì˜ í™•ë¥ ì€ 1ì— ê°€ê¹Œìš¸ ê²ƒì´ë‹¤\n",
    "\n",
    "**But ì˜¤ë‹µ (say ì´ì™¸ì˜ ë‹¨ì–´)ì„ ì…ë ¥í•˜ë©´ ì–´ë–¤ ê²°ê³¼ê°€ ë‚˜ì˜¬ì§€ì— ëŒ€í•´ì„œë„ ì²˜ë¦¬í•´ì¤˜ì•¼í•¨!**\n",
    "\n",
    "ìš°ë¦¬ì˜ ëª©ì  :\n",
    "1. ê¸ì •ì  ì˜ˆ (\"say\")ì— ëŒ€í•´ì„œëŠ” Sigmoid ì˜ ì¶œë ¥ì´ 1ì— ê°€ê¹ê²Œ \n",
    "2. ë¶€ì •ì  ì˜ˆ (\"say\" ì™¸ ë‹¨ì–´)ì— ëŒ€í•´ì„œëŠ” Sigmoidì˜ ì¶œë ¥ì´ 0ì— ê°€ê¹ê²Œ\n",
    "\n",
    "`ì–´ë–»ê²Œ 2ë²ˆì„ êµ¬í˜„í•  ìˆ˜ ìˆì„ê¹Œ?`\n",
    "\n",
    "ëª¨ë“  ë¶€ì •ì  ì˜ˆë¥¼ ëŒ€ìƒìœ¼ë¡œ ì´ì§„ ë¶„ë¥˜ í•™ìŠµ ? \n",
    "**No** ì´ì „ê³¼ ë§ˆì°¬ê°€ì§€ë¡œ high cost!\n",
    "\n",
    "**ê·¼ì‚¬ì ì¸ í•´ë‹µìœ¼ë¡œ ë¶€ì •ì  ì˜ˆë¥¼ ëª‡ ê°œ ì„ íƒí•œë‹¤. ì¦‰, ì ì€ ìˆ˜ì˜ ë¶€ì •ì  ì˜ˆë¥¼ ìƒ˜í”Œë§í•´ ì‚¬ìš©í•œë‹¤. (Negative Sampling)**\n",
    "\n",
    "---\n",
    "### Summary\n",
    "1. ë„¤ê±°í‹°ë¸Œ ìƒ˜í”Œë§ ê¸°ë²•ì€ ê¸ì •ì  ì˜ˆë¥¼ íƒ€ê¹ƒìœ¼ë¡œ í•œ ê²½ìš°ì˜ ì†ì‹¤ì„ êµ¬í•œë‹¤. \n",
    "2. ë™ì‹œì— ë¶€ì •ì  ì˜ˆë¥¼ ëª‡ ê°œ ìƒ˜í”Œë§(ì„ ë³„)í•˜ì—¬ ê° ë¶€ì •ì  ì˜ˆì— ëŒ€í•˜ì—¬ ì†ì‹¤ì„ êµ¬í•œë‹¤. \n",
    "3. (1 + 2) ê° ì†ì‹¤ì„ ë”í•œ ê°’ì„ ìµœì¢… ì†ì‹¤ë¡œ ì •í•œë‹¤.\n",
    "\n",
    "### Example\n",
    "\n",
    "1. say ... 1\n",
    "2. hello ... 0 K ... 0\n",
    "3. sum(Loss)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ë„¤ê±°í‹°ë¸Œ ìƒ˜í”Œë§ì˜ ìƒ˜í”Œë§ ê¸°ë²•\n",
    "\n",
    "**ê·¸ë ‡ë‹¤ë©´ ë¶€ì •ì  ì˜ˆë¥¼ ì–´ë–»ê²Œ ìƒ˜í”Œë§ í•˜ëŠ”ê°€?**\n",
    "\n",
    "ë¬´ì‘ìœ„ ìƒ˜í”Œë§? \n",
    "\n",
    "    No! í¬ì†Œí•œ ë‹¨ì–´ë§Œ ìƒ˜í”Œë§ë˜ì—ˆë‹¤ë©´ ê²°ê³¼ê°€ ë‚˜ë¹ ì§ˆ ê²ƒ\n",
    "\n",
    "ë§ë­‰ì¹˜ì˜ í†µê³„ ë°ì´í„°ë¥¼ ê¸°ì´ˆë¡œ ìƒ˜í”Œë§í•´ë³´ì. \n",
    "\n",
    "**ë§ë­‰ì¹˜ì—ì„œ ìì£¼ ë“±ì¥í•˜ëŠ” ë‹¨ì–´ë¥¼ ë§ì´ ì¶”ì¶œí•˜ê³  ë“œë¬¼ê²Œ ë“±ì¥í•˜ëŠ” ë‹¨ì–´ë¥¼ ì ê²Œ ì¶”ì¶œí•˜ì**\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "6"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "# 0-9 ìˆ«ì ì¤‘ í•˜ë‚˜ ë¬´ì‘ìœ„ ìƒ˜í”Œë§\n",
    "np.random.choice(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'goodbye'"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# ë‹¨ì–´ì—ì„œë„ ë§ˆì°¬ê°€ì§€\n",
    "words = ['you', 'say', 'goodbye' ,'I', 'hello', '.']\n",
    "np.random.choice(words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['.' 'hello' 'you' '.' 'goodbye']\n",
      "['.' 'you' 'goodbye' 'say' 'I']\n",
      "I\n"
     ]
    }
   ],
   "source": [
    "# ì¤‘ë³µ í—ˆìš© ì¶”ì¶œ\n",
    "print(np.random.choice(words, size = 5))\n",
    "# 5ê°œë§Œ ë¬´ì‘ìœ„ ìƒ˜í”Œë§ (ì¤‘ë³µ ì—†ìŒ)\n",
    "print(np.random.choice(words, size = 5, replace = False))\n",
    "# í™•ë¥ ë¶„í¬ì— ë”°ë¼ ìƒ˜í”Œë§\n",
    "p = [0.5, 0.1, 0.05, 0.2, 0.05, 0.1]\n",
    "print(np.random.choice(words, p=p)) # ì¸ìˆ˜ pì— í™•ë¥ ë¶„í¬ ë¦¬ìŠ¤íŠ¸ë¥¼ ì§€ì •í•˜ë©´ í™•ë¥ ë¶„í¬ëŒ€ë¡œ ìƒ˜í”Œë§í•œë‹¤!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "í•œí¸, word2vecì˜ ë„¤ê±°í‹°ë¸Œ ìƒ˜í”Œë§ì—ì„œëŠ” ì•ì˜ í™•ë¥ ë¶„í¬ (p)ì— 0.75ë¥¼ ì œê³±í•˜ë¼ê³  ê¶Œê³ í•œë‹¤.\n",
    "\n",
    "**why?**  <u>0.75 ì œê³±ì„ í•¨ìœ¼ë¡œì¨ ì¶œí˜„ í™•ë¥ ì´ ë‚®ì€ ë‹¨ì–´ë¥¼ ë²„ë¦¬ì§€ ì•Šê¸° ìœ„í•´</u>\n",
    "\n",
    "**0.75 ë¼ëŠ” ìˆ˜ì¹˜ì— ëŒ€í•œ ì´ë¡ ì  ê·¼ê±°ëŠ” ì—†ë‹¤**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.64196878 0.33150408 0.02652714]\n"
     ]
    }
   ],
   "source": [
    "p = [0.7, 0.29, 0.01]\n",
    "new_p = np.power(p, 0.75)\n",
    "new_p /= np.sum(new_p)\n",
    "print(new_p)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "0.75 ì œê³±ì„ í†µí•´ ë‚®ì€ í™•ë¥ ì´ ì•½ê°„ ìƒìŠ¹í•¨"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "class UnigramSampler:\n",
    "    # ì´ˆê¸°í™” ì‹œì— 3ê°œì˜ ì¸ìˆ˜ë¥¼ ë°›ëŠ”ë‹¤\n",
    "    # ë‹¨ì–´ ID ëª©ë¡, í™•ë¥ ë¶„í¬ì— ì œê³±í•  ê°’, ë¶€ì •ì  ì˜ˆì‹œ ìƒ˜í”Œë§í•  ê°œìˆ˜\n",
    "    def __init__(self, corpus, power, sample_size): \n",
    "        self.sample_size = sample_size\n",
    "        self.vocab_size = None\n",
    "        self.word_p = None\n",
    "\n",
    "        # ë‹¨ì–´ ë¹ˆë„ ì‚°ì¶œ\n",
    "        counts = collections.Counter()\n",
    "        for word_id in corpus:\n",
    "            counts[word_id] += 1\n",
    "        \n",
    "        vocab_size = len(counts)\n",
    "        self.vocab_size = vocab_size\n",
    "\n",
    "        self.word_p = np.zeros(vocab_size)\n",
    "        for i in range(vocab_size):\n",
    "            self.word_p[i] = counts[i]\n",
    "        \n",
    "        # ë‹¨ì–´ì˜ ë¹ˆë„ ê¸°ì¤€ í™•ë¥  ë¶„í¬ ì‚°ì¶œ\n",
    "        self.word_p = np.power(self.word_p, power)\n",
    "        self.word_p /= np.sum(self.word_p)\n",
    "        \n",
    "    \n",
    "    # targetìœ¼ë¡œ ì§€ì •í•œ ë‹¨ì–´ë¥¼ ê¸ì •ì  ì˜ˆë¡œ í•´ì„í•˜ê³ , ê·¸ ì™¸ì˜ ë‹¨ì–´ IDë¥¼ ìƒ˜í”Œë§\n",
    "    def get_negative_sample(self, target):\n",
    "        batch_size = target.shape[0]\n",
    "        GPU = False\n",
    "        if not GPU:\n",
    "            negative_sample = np.zeros((batch_size, self.sample_size), dtype=np.int32)\n",
    "\n",
    "            for i in range(batch_size):\n",
    "                p = self.word_p.copy()\n",
    "                target_idx = target[i]\n",
    "                p[target_idx] = 0\n",
    "                p /= p.sum()\n",
    "                negative_sample[i, :] = np.random.choice(self.vocab_size, size=self.sample_size, replace=False, p=p)\n",
    "        else:\n",
    "            # GPU(cupyï¼‰ë¡œ ê³„ì‚°í•  ë•ŒëŠ” ì†ë„ë¥¼ ìš°ì„ í•œë‹¤.\n",
    "            # ë¶€ì •ì  ì˜ˆì— íƒ€ê¹ƒì´ í¬í•¨ë  ìˆ˜ ìˆë‹¤.\n",
    "            negative_sample = np.random.choice(self.vocab_size, size=(batch_size, self.sample_size),\n",
    "                                               replace=True, p=self.word_p)\n",
    "\n",
    "        return negative_sample"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### í™œìš© ì˜ˆì‹œ"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "import collections"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0 2]\n",
      " [0 1]\n",
      " [4 1]]\n"
     ]
    }
   ],
   "source": [
    "corpus = np.array([0, 1, 2, 3, 4, 1, 2, 3])\n",
    "power = 0.75\n",
    "sample_size = 2\n",
    "\n",
    "sampler = UnigramSampler(corpus, power, sample_size)\n",
    "target = np.array([1,3,0])\n",
    "negative_sample = sampler.get_negative_sample(target)\n",
    "print(negative_sample)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- target 1ì— í•´ë‹¹í•˜ëŠ” ë¶€ì •ì  ì˜ˆì‹œ 2 0\n",
    "- target 3ì— í•´ë‹¹í•˜ëŠ” ë¶€ì •ì  ì˜ˆì‹œ ìƒ˜í”Œë§ 1 0\n",
    "- target 0ì— í•´ë‹¹í•˜ëŠ” ë¶€ì •ì  ì˜ˆì‹œ ìƒ˜í”Œë§ 1 3"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ë„¤ê±°í‹°ë¸Œ ìƒ˜í”Œë§ êµ¬í˜„"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "class NegativeSamplingLoss:\n",
    "    # ì¶œë ¥ ê°€ì¤‘ì¹˜ W, ë§ë­‰ì¹˜ ID ë¦¬ìŠ¤íŠ¸, í™•ë¥ ë¶„í¬ì— ì œê³±í•  ê°’, ìƒ˜í”Œë§ íšŸìˆ˜\n",
    "    def __init__(self, W, corpus, power = 0.75, sample_size = 5):\n",
    "        self.sample_size = sample_size\n",
    "        self.sampler = UnigramSampler(corpus, power, sample_size)\n",
    "\n",
    "        # ì›í•˜ëŠ” ê³„ì¸µì„ ë¦¬ìŠ¤íŠ¸ë¡œ ë³´ê´€\n",
    "        self.loss_layers = [SigmoidWithLoss() for _ in range(sample_size + 1)] # ë¶€ì •ì  ì˜ˆì‹œ(sample_size) + ê¸ì •ì  ì˜ˆì‹œ (1)\n",
    "        self.embed_dot_layers = [EmbeddingDot(W) for _ in range(sample_size + 1)]\n",
    "        \n",
    "        self.params, self.grads = [],[]\n",
    "        for layer in self.embed_dot_layers:\n",
    "            self.params += layer.params\n",
    "            self.grads += layer.grads\n",
    "    \n",
    "    def forward(self, h, target):\n",
    "        batch_size = target.shape[0]\n",
    "        negative_sample = self.sampler.get_negative_sample(target) # ë¶€ì •ì  ì˜ˆë¥¼ ìƒ˜í”Œë§í•˜ì—¬ ë³€ìˆ˜ì— ì €ì¥\n",
    "        \n",
    "        #ê¸ì •ì  ì˜ˆ ìˆœì „íŒŒ. 0ë²ˆì§¸ ê³„ì¸µ\n",
    "        score = self.embed_dot_layers[0].forward(h,target)\n",
    "        correct_label = np.ones(batch_size, dtype=np.int32) # 1\n",
    "        loss = self.loss_layers[0].forward(score, correct_label)\n",
    "        \n",
    "        #ë¶€ì •ì  ì˜ˆ ìˆœì „íŒŒ \n",
    "        negative_label = np.zeros(batch_size, dtype=np.int32) # 0\n",
    "        for i in range(self.sample_size):\n",
    "            negative_target = negative_sample[:,i]\n",
    "            score = self.embed_dot_layers[1 + i].forward(h, negative_target) \n",
    "            loss += self.loss_layers[1 + i].forward(score, negative_label)\n",
    "\n",
    "        return loss\n",
    "    \n",
    "    def backward(self, dout=1):\n",
    "        dh = 0\n",
    "        for l0, l1 in zip(self.loss_layers, self.embed_dot_layers):\n",
    "            dscore = l0.backward(dout)\n",
    "            dh += l1.backward(dscore)\n",
    "            \n",
    "        return dh"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ê°œì„ íŒ word2vec í•™ìŠµ"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## CBOW ëª¨ë¸ êµ¬í˜„"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CBOW:\n",
    "    # ì–´íœ˜ìˆ˜, ì€ë‹‰ì¸µ ë‰´ëŸ°ìˆ˜, ë§¥ë½ í¬ê¸°, ë‹¨ì–´ ID ëª©ë¡\n",
    "    def __init__(self, vocab_size, hidden_size, window_size, corpus):\n",
    "        V, H = vocab_size, hidden_size\n",
    "\n",
    "        # ê°€ì¤‘ì¹˜ ì´ˆê¸°í™”\n",
    "        W_in = 0.01 * np.random.randn(V, H).astype('f')\n",
    "        W_out = 0.01 * np.random.randn(V, H).astype('f') # embedding layerë¥¼ ì‚¬ìš©í•˜ë¯€ë¡œ ë‘˜ì˜ í˜•ìƒì´ ê°™ë‹¤.\n",
    "\n",
    "        # ê³„ì¸µ ìƒì„±\n",
    "        self.in_layers = []\n",
    "        for i in range(2 * window_size):\n",
    "            layer = Embedding(W_in)  # Embedding ê³„ì¸µ 2*window_sizeë§Œí¼ ì‚¬ìš©\n",
    "            self.in_layers.append(layer) # ë°°ì—´ë¡œ ë³´ê´€\n",
    "        self.ns_loss = NegativeSamplingLoss(W_out, corpus, power=0.75, sample_size=5)\n",
    "\n",
    "        # ëª¨ë“  ê°€ì¤‘ì¹˜ì™€ ê¸°ìš¸ê¸°ë¥¼ ë°°ì—´ì— ëª¨ì€ë‹¤.\n",
    "        layers = self.in_layers + [self.ns_loss]\n",
    "        self.params, self.grads = [], []\n",
    "        for layer in layers:\n",
    "            self.params += layer.params\n",
    "            self.grads += layer.grads\n",
    "\n",
    "        # ì¸ìŠ¤í„´ìŠ¤ ë³€ìˆ˜ì— ë‹¨ì–´ì˜ ë¶„ì‚° í‘œí˜„ì„ ì €ì¥í•œë‹¤.\n",
    "        self.word_vecs = W_in\n",
    "\n",
    "    def forward(self, contexts, target):\n",
    "        h = 0\n",
    "        for i, layer in enumerate(self.in_layers):\n",
    "            h += layer.forward(contexts[:, i])\n",
    "        h *= 1 / len(self.in_layers)\n",
    "        loss = self.ns_loss.forward(h, target) # lossê°€ ë‹¤ë¥´ë‹¤! ê°„ê²°í•´ì§\n",
    "        return loss\n",
    "\n",
    "    def backward(self, dout=1):\n",
    "        dout = self.ns_loss.backward(dout)\n",
    "        dout *= 1 / len(self.in_layers)\n",
    "        for layer in self.in_layers:\n",
    "            layer.backward(dout)\n",
    "        return None"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## CBOW ëª¨ë¸ í•™ìŠµ ì½”ë“œ"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.append(\"..\")\n",
    "import numpy as np\n",
    "from common_wj import config\n",
    "# GPUì—ì„œ ì‹¤í–‰í•˜ë ¤ë©´ ì•„ë˜ ì£¼ì„ì„ í•´ì œí•˜ì„¸ìš”(CuPy í•„ìš”).\n",
    "# ===============================================\n",
    "# config.GPU = True\n",
    "# ===============================================\n",
    "import pickle\n",
    "from common_wj.trainer import Trainer\n",
    "from common_wj.optimizer import Adam\n",
    "from cbow import CBOW\n",
    "from common_wj.util import create_contexts_target, to_cpu, to_gpu\n",
    "from dataset import ptb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "| epoch 1 |  iter 1 / 9295 | time 0[s] | loss 4.16\n",
      "| epoch 1 |  iter 21 / 9295 | time 1[s] | loss 4.16\n",
      "| epoch 1 |  iter 41 / 9295 | time 2[s] | loss 4.15\n",
      "| epoch 1 |  iter 61 / 9295 | time 3[s] | loss 4.12\n",
      "| epoch 1 |  iter 81 / 9295 | time 4[s] | loss 4.05\n",
      "| epoch 1 |  iter 101 / 9295 | time 5[s] | loss 3.92\n",
      "| epoch 1 |  iter 121 / 9295 | time 6[s] | loss 3.78\n",
      "| epoch 1 |  iter 141 / 9295 | time 7[s] | loss 3.63\n",
      "| epoch 1 |  iter 161 / 9295 | time 8[s] | loss 3.49\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-2-59fda5a4c167>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     21\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     22\u001b[0m \u001b[0;31m# í•™ìŠµ ì‹œì‘\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 23\u001b[0;31m \u001b[0mtrainer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcontexts\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtarget\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmax_epoch\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch_size\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     24\u001b[0m \u001b[0mtrainer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mplot\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     25\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/Google á„ƒá…³á„…á…¡á„‹á…µá„‡á…³/2020/Study/[2019.12-] Keracorn/Keracorn-NLP-Study/common_wj/trainer.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, x, t, max_epoch, batch_size, max_grad, eval_interval)\u001b[0m\n\u001b[1;32m     35\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     36\u001b[0m                 \u001b[0;31m# å‹¾é…ã‚’æ±‚ã‚ã€ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿ã‚’æ›´æ–°\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 37\u001b[0;31m                 \u001b[0mloss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbatch_x\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch_t\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     38\u001b[0m                 \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     39\u001b[0m                 \u001b[0mparams\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgrads\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mremove_duplicate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mparams\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgrads\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# å…±æœ‰ã•ã‚ŒãŸé‡ã¿ã‚’1ã¤ã«é›†ç´„\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/Google á„ƒá…³á„…á…¡á„‹á…µá„‡á…³/2020/Study/[2019.12-] Keracorn/Keracorn-NLP-Study/ch04/cbow.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, contexts, target)\u001b[0m\n\u001b[1;32m     38\u001b[0m             \u001b[0mh\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0mlayer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcontexts\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     39\u001b[0m         \u001b[0mh\u001b[0m \u001b[0;34m*=\u001b[0m \u001b[0;36m1\u001b[0m \u001b[0;34m/\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0min_layers\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 40\u001b[0;31m         \u001b[0mloss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mns_loss\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mh\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtarget\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;31m# lossê°€ ë‹¤ë¥´ë‹¤! ê°„ê²°í•´ì§\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     41\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mloss\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     42\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/Google á„ƒá…³á„…á…¡á„‹á…µá„‡á…³/2020/Study/[2019.12-] Keracorn/Keracorn-NLP-Study/ch04/negative_sampling_layer.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, h, target)\u001b[0m\n\u001b[1;32m     97\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mh\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtarget\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     98\u001b[0m         \u001b[0mbatch_size\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtarget\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 99\u001b[0;31m         \u001b[0mnegative_sample\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msampler\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_negative_sample\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtarget\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# ë¶€ì •ì  ì˜ˆë¥¼ ìƒ˜í”Œë§í•˜ì—¬ ë³€ìˆ˜ì— ì €ì¥\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    100\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    101\u001b[0m         \u001b[0;31m# ê¸ì •ì  ì˜ˆ ìˆœì „íŒŒ. 0ë²ˆì§¸ ê³„ì¸µ\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/Google á„ƒá…³á„…á…¡á„‹á…µá„‡á…³/2020/Study/[2019.12-] Keracorn/Keracorn-NLP-Study/ch04/negative_sampling_layer.py\u001b[0m in \u001b[0;36mget_negative_sample\u001b[0;34m(self, target)\u001b[0m\n\u001b[1;32m     70\u001b[0m                 \u001b[0mp\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mtarget_idx\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     71\u001b[0m                 \u001b[0mp\u001b[0m \u001b[0;34m/=\u001b[0m \u001b[0mp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msum\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 72\u001b[0;31m                 \u001b[0mnegative_sample\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m:\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrandom\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mchoice\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvocab_size\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msize\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msample_size\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreplace\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mp\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mp\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     73\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     74\u001b[0m             \u001b[0;31m# GPU(cupyï¼‰ë¡œ ê³„ì‚°í•  ë•ŒëŠ” ì†ë„ë¥¼ ìš°ì„ í•œë‹¤.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32mmtrand.pyx\u001b[0m in \u001b[0;36mnumpy.random.mtrand.RandomState.choice\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;32m<__array_function__ internals>\u001b[0m in \u001b[0;36mcumsum\u001b[0;34m(*args, **kwargs)\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# í•˜ì´í¼íŒŒë¼ë¯¸í„° ì„¤ì •\n",
    "window_size = 5\n",
    "hidden_size = 100\n",
    "batch_size = 100\n",
    "max_epoch = 10\n",
    "\n",
    "# ë°ì´í„° ì½ê¸°\n",
    "corpus, word_to_id, id_to_word = ptb.load_data('train')\n",
    "vocab_size = len(word_to_id)\n",
    "\n",
    "contexts, target = create_contexts_target(corpus, window_size)\n",
    "\n",
    "if config.GPU:\n",
    "    contexts, target = to_gpu(contexts), to_gpu(target)\n",
    "\n",
    "# ëª¨ë¸ ë“± ìƒì„±\n",
    "model = CBOW(vocab_size, hidden_size, window_size, corpus)\n",
    "# model = SkipGram(vocab_size, hidden_size, window_size, corpus)\n",
    "optimizer = Adam()\n",
    "trainer = Trainer(model, optimizer)\n",
    "\n",
    "# í•™ìŠµ ì‹œì‘\n",
    "trainer.fit(contexts, target, max_epoch, batch_size)\n",
    "trainer.plot()\n",
    "\n",
    "# ë‚˜ì¤‘ì— ì‚¬ìš©í•  ìˆ˜ ìˆë„ë¡ í•„ìš”í•œ ë°ì´í„° ì €ì¥\n",
    "word_vecs = model.word_vecs\n",
    "if config.GPU:\n",
    "    word_vecs = to_cpu(word_vecs)\n",
    "params = {}\n",
    "params['word_vecs'] = word_vecs.astype(np.float16)\n",
    "params['word_to_id'] = word_to_id\n",
    "params['id_to_word'] = id_to_word\n",
    "pkl_file = 'cbow_params.pkl'  # or 'skipgram_params.pkl'\n",
    "with open(pkl_file, 'wb') as f:\n",
    "    pickle.dump(params, f, -1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Word analogy test\n",
    "\n",
    "    Syntactic:\n",
    "    e.g., bad : worst = good : best\n",
    "    Semantics:\n",
    "    e.g., Seoul : Korea = Tokyo : Japan\n",
    "\n",
    "**Why does this imply that LMs are well trained? What does it have to do with vector arithmetic? When does this hold?**\n",
    "\n",
    "--> word2vecì„ ìˆ˜í•™ì ìœ¼ë¡œ ì ‘ê·¼í•´ì„œ ì„±ëŠ¥ì„ ë¶„ì„í•˜ëŠ” ë…¼ë¬¸ì´ 2019ë…„ ACLì— ë°œí‘œë˜ì—ˆë‹¤.\n",
    "\n",
    "#### Ethayarajh et al., Towards Understanding Linear Word Analogies, ACL 2019\n",
    "\n",
    "word analogy ë¥¼ í•¨ìˆ˜ë¡œ ì ‘ê·¼!\n",
    "\n",
    "Analogy ğ‘“ ê°€ set of ordered pairs ğ‘† ìƒì˜ invertible transformationì´ë‹¤\n",
    "\n",
    "\n",
    "seoul, Korea -> í•œêµ­ `ìˆ˜ë„` = ì„œìš¸ / function = \"ì˜ ìˆ˜ë„\"!\n",
    "\n",
    "https://kawine.github.io/blog/nlp/2019/06/21/word-analogies.html\n",
    "\n",
    "<img src=\"../imgs/word_analogy.png\" width=\"1000\" align='left'>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  },
  "toc-autonumbering": true
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
